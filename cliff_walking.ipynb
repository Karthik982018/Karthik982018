{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cliff_walking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthik982018/Karthik982018/blob/main/cliff_walking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "\n",
        "# COMP 532 â€“ Machine Learning and Bio-Inspired Optimisation\n",
        "# Assignment 1 Reinforcement Learning\n",
        "\n",
        "# submitted by:\n",
        "# Aby Tom - 201601516 - sgatom@liverpool.ac.uk\n",
        "# Alwin Joseph Christopher - 201594340 - sgachri4@liverpool.ac.uk\n",
        "# Jewel Sunny - 201603769 -\tsgjsunny@liverpool.ac.uk\n",
        "# Karthik Valiyaparambil Subramanian - 201586749 -  sgkvaliy@liverpool.ac.uk\n",
        "\n",
        "\n",
        "# note:running the code in colab will fetch all the required results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# \n",
        "\n",
        "\n",
        "\n",
        "# importing openAI gym \n",
        "import gym\n",
        "# importing the cliff walking environment\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# creating the environment\n",
        "env=CliffWalkingEnv()\n",
        "\n",
        "# defines the sarsa algorithm\n",
        "# initAct is the initial action values for the states \n",
        "# env is the environment\n",
        "# epsilon is the exploration coefficient\n",
        "# discount is the discounting factor which is 1 since episodic\n",
        "# episodes is the number of episodes\n",
        "def sarsa(initAct,env,epsilon,step,discount,episodes):\n",
        "\n",
        "  action_value=initAct\n",
        "  rewards = np.empty(episodes)\n",
        "\n",
        "  \n",
        "  \n",
        "  # iterating through the episode\n",
        "  for i in range(episodes):\n",
        "    \n",
        "    # reset the state to init value\n",
        "    env.reset()\n",
        "    # current state\n",
        "    state=env.s\n",
        "    \n",
        "    p=np.random.random()\n",
        "\n",
        "    actionChoice=0\n",
        "    # epsilon-greedy\n",
        "    if p<epsilon:\n",
        "      actionChoice =random.randint(0, 3)\n",
        "      \n",
        "    else:\n",
        "      actionChoice=np.argmax(action_value[state])\n",
        "    r=[]\n",
        "    # defining the runs\n",
        "    while True:\n",
        "      # taking the next step\n",
        "      next_state, reward, is_terminal, t_prob = env.step(actionChoice)\n",
        "      p=np.random.random()\n",
        "      a=0\n",
        "      # epsilon greedy\n",
        "      if p<epsilon:\n",
        "        a=random.randint(0, 3)\n",
        "      \n",
        "      else:\n",
        "        \n",
        "        a=np.argmax(action_value[state])\n",
        "\n",
        "      # updating the action value using sarsa\n",
        "      action_value[state][actionChoice]+=step*(reward+(discount*action_value[next_state][a])-action_value[state][actionChoice])\n",
        "      \n",
        "      state=next_state\n",
        "      actionChoice=a\n",
        "      \n",
        "\n",
        "      if is_terminal:\n",
        "        break\n",
        "        \n",
        "      r.append(reward)\n",
        "\n",
        "    sum=np.sum(r)     \n",
        "    rewards[i]=sum\n",
        "  # returning the reward and the final action value\n",
        "  return rewards,action_value\n",
        "\n",
        "\n",
        "# defines the q-learning algorithm\n",
        "# initAct is the initial action values for the states \n",
        "# env is the environment\n",
        "# epsilon is the exploration coefficient\n",
        "# discount is the discounting factor which is 1 since episodic\n",
        "# episodes is the number of episodes\n",
        "\n",
        "def qLearning(initAct,env,epsilon,step,discount,episodes):\n",
        "\n",
        "  action_value=initAct\n",
        "  rewards = np.empty(episodes)\n",
        "\n",
        "  \n",
        "  \n",
        "  # iterating through the episodes\n",
        "  for i in range(episodes):\n",
        "    \n",
        "    # reset to the initial state\n",
        "    env.reset()\n",
        "    # current state\n",
        "    state=env.s\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    r=[]\n",
        "    # defining the runs\n",
        "    while True:\n",
        "      \n",
        "      \n",
        "      p=np.random.random()\n",
        "      a=0\n",
        "      # epsilon greedy\n",
        "      if p<epsilon:\n",
        "        a=random.randint(0, 3)\n",
        "      \n",
        "      else:\n",
        "        \n",
        "        a=np.argmax(action_value[state])\n",
        "      next_state, reward, is_terminal, t_prob = env.step(a)\n",
        "      # updating the value using q-learning\n",
        "      action_value[state][a]+=step*(reward+(discount*max(action_value[next_state]))-action_value[state][a])\n",
        "      \n",
        "      state=next_state\n",
        "      \n",
        "      \n",
        "\n",
        "      if is_terminal:\n",
        "        break\n",
        "\n",
        "\n",
        "    sum=np.sum(r)     \n",
        "    rewards[i]=sum\n",
        "  # returning the rewards and the final action values\n",
        "  return rewards,action_value\n",
        "# smoothing the rewards\n",
        "def smoothRewards(rewards):\n",
        "\n",
        "  cum_rewards = []\n",
        "\n",
        "  rewards_mean = np.array(rewards).mean()\n",
        "\n",
        "  rewards_std = np.array(rewards).std()\n",
        "\n",
        "  count = 0 # used to determine the batches\n",
        "\n",
        "  cur_reward = 0 # accumulate reward for the batch\n",
        "\n",
        "  for cache in rewards:\n",
        "\n",
        "      count = count + 1\n",
        "\n",
        "      cur_reward += cache\n",
        "\n",
        "      if(count == 10):\n",
        "\n",
        "          # normalize the sample\n",
        "\n",
        "          normalized_reward = (cur_reward - rewards_mean)/rewards_std\n",
        "\n",
        "          cum_rewards.append(normalized_reward)\n",
        "\n",
        "          cur_reward = 0\n",
        "\n",
        "          count = 0\n",
        "\n",
        "  return cum_rewards\n",
        "\n",
        "# initialise the action values and and runs sarsa and q-learning and plots the output\n",
        "def run(env,epsilon,step,discount,episodes):\n",
        "  # initialising the action values\n",
        "  action_value_sarsa=[]\n",
        "  np.random.seed(13)\n",
        "  for i in range(0,env.nS):\n",
        "    qa=[]\n",
        "    for j in range(env.nA):\n",
        "      r=np.random.random()\n",
        "      qa.append(r)\n",
        "    action_value_sarsa.append(qa)\n",
        "  term_action=[]\n",
        "  for i in range(env.nA):\n",
        "    term_action.append(0)\n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "  action_value_sarsa.append(term_action)\n",
        "  action_value_qlearning=np.copy(action_value_sarsa)\n",
        "  # run sarsa\n",
        "  sarsaRewards,average1=sarsa(action_value_sarsa,env,epsilon,step,discount,episodes)\n",
        "  # run q-learning\n",
        "  qRewards,average2=sarsa(action_value_qlearning,env,epsilon,step,discount,episodes)\n",
        "  \n",
        "  # applying smoothening\n",
        "  cumSarsa=smoothRewards(sarsaRewards)\n",
        "  cumQLearn=smoothRewards(qRewards)\n",
        "\n",
        "  # plotting\n",
        "  plt.figure(figsize = (12, 8))\n",
        "  plt.title(\"epsilon value:{} \".format(epsilon))\n",
        "\n",
        "  plt.xlabel(\"batches of episodes(sample size:10)\")\n",
        "  plt.ylabel(\"cumulative rewards\")\n",
        "\n",
        "  plt.plot(cumSarsa, label ='sarsa')\n",
        "  plt.plot(cumQLearn, label ='Q-learning')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "     \n",
        "\n",
        "run(env,.01,.1,1,500)\n",
        "run(env,.1,.1,1,500)\n",
        "run(env,.2,.1,1,500)\n",
        "\n",
        "      \n"
      ],
      "metadata": {
        "id": "wdasxy7n7dDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}